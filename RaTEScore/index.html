<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  /* h1 {
    font-weight:300;
  } */
  h1 {
    font-weight:300;
  display: flex;
  align-items: center; /* 保证图片和文字垂直居中对齐 */
  }

  .title  .title-image {
    height: 1em; /* 使用 em 单位使图片高度与文字大小相匹配 */
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }

  .image-container {
    display: flex; /* 使用flexbox布局 */
    justify-content: space-around; /* 图片间隔均匀分布 */
  }
  .image-container img {
    height: 300px; /* 控制所有图片的高度 */
    width: auto; /* 宽度自适应，保持图片比例 */
  }
  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  .right-align {
    text-align: right;
  }

  figure {
    text-align: center;
  }

  figcaption {
    font-style: italic;
    margin-top: 5px;
  }
</style>

	<title>RaTEScore: A Metric for Radiology Report Generation</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:38px;"> <img src="resources/logo.png" alt="Description of Image" style="height: 45px; vertical-align: text-top;">RaTEScore: A Metric for Radiology Report Generation</span><br><br><br>
	</center>
  </table>
  
	<table align="center" width="560px">
            <tbody><tr>
      
                    <td align="center" width="160px">
              <center>
                <span style="font-size:18px"><a href="https://angelakeke.github.io/">Weike Zhao</a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:18px"><a href="https://chaoyi-wu.github.io/">Chaoyi Wu</a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:18px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a><sup>1,2</sup></span>
                </center>
                </td>
              
              
		        
          </tr>
        </tbody></table><br>

    <table align="center" width="560px">
            <tbody><tr>
                <td align="center" width="300px">
              <center>
                <span style="font-size:18px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2</sup></span>
                </center>
              </td>
                    <td align="center" width="300px">
              <center>
                <span style="font-size:18px"><a href="https://mediabrain.sjtu.edu.cn/">Yanfeng Wang</a><sup>1,2,<img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span>
                </center>
              </td>
              <td align="center" width="300px">
              <center>
                    <span style="font-size:18px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2,<img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span>
                  </td>
                  
        </tr></tbody></table><br>


	  <table align="center" width="650px">

            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:14px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>Shanghai Jiao Tong University</span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Laboratory</span>
                </center>
                </td>
        </tr></tbody></table>

        <br>
    
        <table style="width: 800px; border-collapse: collapse;">
          <tr>
            <td style="width: 20%; text-align: center;">
              <a href='https://github.com/Angelakeke/RaTEScore'>
                <img src='https://img.shields.io/badge/Github-Repository-blueviolet' alt='website URL'>
              </a>
            </td>
            <td style="width: 20%; text-align: center;">
              <a href='https://huggingface.co/Angelakeke/RaTE-NER'>
                <img src='https://img.shields.io/badge/RaTENER-Model&Demo-blue' alt='RaTENER Model & Demo'>
              </a>
            </td>
            <td style="width: 20%; text-align: center;">
              <a href='https://huggingface.co/datasets/Angelakeke/RaTE-NER'>
                <img src='https://img.shields.io/badge/RaTENER-Dataset-blue' alt='RaTENER Dataset'>
              </a>
            </td>
            <td style="width: 20%; text-align: center;">
              <a href='https://huggingface.co/datasets/Angelakeke/RaTE-Eval'>
                <img src='https://img.shields.io/badge/RaTEEval-Benchmark-green' alt='RaTEEval Benchmark'>
              </a>
            </td>
            <td style="width: 20%; text-align: center;">
              <a href='https://arxiv.org/pdf/2406.16845'>
                <img src='https://img.shields.io/badge/Paper-PDF-red' alt='Paper PDF'>
              </a>
            </td>
          </tr>
        </table>

	<!-- <table align="center" width="800px">
    <a href='https://angelakeke.github.io/RaTEScore/'><img src='https://img.shields.io/badge/website-URL-blueviolet'></a>
    <a href='https://huggingface.co/Angelakeke/RaTE-NER'><img src='https://img.shields.io/badge/RaTENER-Model&Demo-blue'></a>
    <a href='https://huggingface.co/datasets/Angelakeke/RaTE-NER'><img src='https://img.shields.io/badge/RaTENER-Dataset-blue'></a>
    <a href='https://huggingface.co/datasets/Angelakeke/RaTE-Eval'><img src='https://img.shields.io/badge/RaTEEval-Benchmark-green'></a> 
    <a href=''><img src='https://img.shields.io/badge/Paper-PDF-red'></a>
            <!-- <tbody><tr>
              <td align="center" width="140px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/Angelakeke/RaTEScore"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="140px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://arxiv.org/abs/2312.16151"> [arXiv]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="240px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Model <a href="https://huggingface.co/Angelakeke/RaTE-NER-Deberta"> [HuggingFace]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="240px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Dataset <a href="https://huggingface.co/datasets/Angelakeke/RaTE-NER"> [HuggingFace]</a>
                  </span>
                </center>
              </td>
            </tr></tbody> -->
      <!-- </table> --> 
     
      <br><hr>
      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
            <left>
              This paper proposes a new entity-aware lightweight metric for assessing accuracy of generated medical free-form text from AI models.
              Our metric, termed as <strong>Ra</strong>diological Report  <strong>T</strong>ext  <strong>E</strong>valuation (<strong>RaTEScore</strong>), is designed to focus on 
              key medical entities, such as diagnostic outcomes, anatomies, while demonstrating robustness against complex medical synonyms and 
              sensitivity to negation expressions. 
              Technically, we establish a new large-scale medical NER dataset <strong>RaTE-NER</strong> and train an NER model on it. 
              Leveraging it, we decompose complex radiological reports into medical entities. 
              We define the final metric by comparing the similarity based on the entity embeddings computed from language model and their corresponding types, 
              forcing the metrics to focus on clinically critical statements. 
              In experiments, our score demonstrates superior performance on aligning with human preference than other metrics, 
              both on the existing public benchmarks and our new proposed <strong>RaTE-Eval</strong> benchmark.</left></p>

      <br><hr>
      <center><h2> Motivation </h2> </center>
      <p>
        In the literature, four main types of metrics have been adopted to assess the similarity between free-form texts in medical scenarios, as shown in Figure below. 
        These include: 
      </p>

      <p>
        (i) Metrics based on word overlaps, such as BLEU and ROUGE. 
        Although intuitive, these metrics fail to capture negation or synonyms in sentences, thereby neglecting the assessment of semantic factuality; 
      </p>

      <p>
        (ii) Metrics based on embedding similarities, like BERTScore. While achieving better semantic awareness, they do not focus on key medical terms, 
        thus severely overlooking the local correctness of crucial conclusions; 
      </p>

      <p>
        (iii) Metrics based on Named Entity Recognition (NER), such as RadGraph F1 and MEDCON. 
        Although developed specifically for the medical domain, these metrics often fail to merge synonyms and predominantly focus on Chest X-ray reports; 
      </p>

      <p>
        (iv) Metrics relying on large language models (LLMs). While these metrics are better aligned with human preferences, they suffer from potential subjective biases and are prohibitively expensive for large-scale evaluation.
      </p>

      <p>
        <figure>
          <img style="width:700px" src='./resources/teaser.png'>
        </figure>
      </p>
      <p style="text-align:justify; text-justify:inter-ideograph;">
            <left>
              Existing evaluation metrics. We illustrate the limitations of current metrics. Blue boxes represent ground-truth reports; red and yellow boxes indicate correct and incorrect generated reports, respectively. 
              The examples show that these metrics fail to identify opposite meanings and synonyms in the reports and are often disturbed by unrelated information.</left></p>
      <br>

      <hr>
      <center> <h2> Results </h2> </center>
      <p style="font-weight: bold; font-size: large;">
        R1: Results in RaTE-Eval Benchmark -- task 1.
      </p>
      <p>
      <figure>
        <img style="width:700px" src='./resources/result_short.jpeg'>
        <!-- <figcaption> 
          Our metric exhibits the highest Pearson correlation coefficient with the radiologists' scoring. 
          Note that the scores on the horizontal axis are experts counting various types of errors normalized by 
          the potential error types that could occur in the given sentence, and subtracting this normalized score from 1 
          to achieve a positive correlation.</figcaption> -->
      </figure>
    </p>
    <p>
      Our metric exhibits the highest Pearson correlation coefficient with the radiologists' scoring. 
          Note that the scores on the horizontal axis are experts counting various types of errors normalized by 
          the potential error types that could occur in the given sentence, and subtracting this normalized score from 1 
          to achieve a positive correlation.
    </p>



    <p style="font-weight: bold; font-size: large;">
      R2: Results in RaTE-Eval Benchmark -- task 2 & task 3.
    </p>
    <p>
    <figure>
      <img style="width:400px" src='./resources/t1.jpg'>
      <!-- <figcaption> 
        Our metric exhibits the highest Pearson correlation coefficient with the radiologists' scoring. 
        Note that the scores on the horizontal axis are experts counting various types of errors normalized by 
        the potential error types that could occur in the given sentence, and subtracting this normalized score from 1 
        to achieve a positive correlation.</figcaption> -->
    </figure>
  </p>
  <p>
    Correlation coefficients with radiologists and accuracy for whether the synonym sentence can achieve higher scores than
    the antonymous one on Synthetic Reports. In task 2: 
    it can be observed that RaTEScore shows a significantly higher correlation with radiology experts compared to other non-composite metrics, 
    across various measures of correlation. In task 3:
    our model excels in managing synonym and antonym challenges, affirming its robustness in nuanced language processing within a medical context.

  </p>




  <p style="font-weight: bold; font-size: large;">
    R3: Results in ReXVal dataset.
  </p>
  <p>
  <figure>
    <img style="width:450px" src='./resources/rexval.jpg'>
    <!-- <figcaption> 
      Our metric exhibits the highest Pearson correlation coefficient with the radiologists' scoring. 
      Note that the scores on the horizontal axis are experts counting various types of errors normalized by 
      the potential error types that could occur in the given sentence, and subtracting this normalized score from 1 
      to achieve a positive correlation.</figcaption> -->
  </figure>
</p>
<p>
  RaTEScore demonstrated a Kendall correlation coefficient of 0.527 with the error counts, surpassing all existing metrics.
</p>
<br>
<p>For more detailed ablation studies, please refer to our <a href="https://arxiv.org/pdf/2406.16845"> paper</a>.</p>
      <br>

      <hr>
      <center> <h2> General Pipeline </h2> </center>

 <p>
  <figure>
  <img style="width:750px" src='./resources/model.png'>
  <figcaption><strong>Illustration of the Computation of RaTEScore.</strong> Given a reference radiology report \(x\), a candidate radiology report \(\hat{x}\), 
    we first extract the medical entity and the corresponding entity type. Then, we compute the entity embedding and find the maximum cosine similarity. 
    The RaTEScore is computed by the weighted similarity scores that consider the pairwise entity types.
  </figcaption></p></figure>
     
  <p>The key intuition of our proposed RaTEScore is to compare two radiological reports at the entity level. Given two radiological reports, one is the ground truth for reference, denoting as \(x\), and the other candidate for evaluation as \(\hat{x}\). We aim to define a new similarity metric \(S(x, \hat{x})\), better reflecting the clinical consistency between the two.</p>

  <p>As shown in Figure <a href="#fig:model">1</a>, our pipeline contains three major components:
  namely, a medical entity recognition module \(\Phi_{\text{NER}}(\cdot)\),
  a synonym disambuation encoding module \(\Phi_{\text{ENC}}(\cdot)\),
  and a final scoring module \(\Phi_{\text{SIM}}(\cdot)\).</p>
  
  <p>First, we extract the medicial entities from each piece of radiological text, then encode each entity into embeddings that are aware of medical synonyms, formulated as:</p>
  <div>
    \[
    \mathbf{F} = \Phi_{\text{ENC}}(\Phi_{\text{NER}}(x)),
    \]
    where \(\mathbf{F}\) contains a set of entity embeddings.
  </div>
  <div>
    Similarly, we can get \(\mathbf{\hat{F}}\) for \(\hat{x}\). Then, we can calculate the final similarity on the entity embeddings as:
    \[
    S(x, \hat{x}) = \Phi_{\text{SCO}}(\mathbf{F}, \mathbf{\hat{F}}).
    \]
  </div>
  <p>The detail of each components please refer to our  <a href="https://arxiv.org/pdf/2406.16845"> paper</a>.</p>

      <br>

      <hr>
      <center> <h2> RaTE-NER </h2> </center>

      <!-- <div class="image-container">
        <img src="./resources/data.jpg" alt="Image 1">
        <img src="./resources/data_curation_3.png" alt="Image 2">
      </div> -->
      <p>
      <figure>
        <img style="width:400px" src='./resources/data.jpg'>
        <figcaption>RaTE-NER Dataset Statistics: The dataset consists of two data sources: MIMIC-IV and Radiopaedia. 
          # represents specific types of medical entities.  For "Reports" line, the numbers in "()" are number of source reports.   
          For the "Entities" and # lines, the numbers in "()" are counts of non-redundant entities.</figcaption>
      </figure>
    </p>
    <p>
      To facilitate training our medical entity recognition module, we constructed a RaTE-NER dataset, a large-scale, radiological named entity recognition (NER) dataset. 
      This dataset comprises 13,235 manually annotated sentences from 1,816 reports within the MIMIC-IV database, 
      adhering to our predefined entity-labeling framework which spans 9 imaging modalities and 23 anatomical regions, ensuring broad coverage. 
    </p>
    <p>
      Given that reports in MIMIC-IV are more likely to cover common diseases,
      and may not well represent rarer conditions, we further enriched the dataset with 33,605 sentences from the 17432 reports available on 
      Radiopaedia, by leveraging GPT-4 and other medical knowledge libraries to capture intricacies and nuances of less common diseases and abnormalities. 
      More details can be found in the Appendix. 
      We manually labeled 3,529 sentences to create a test set,  as shown in Table, the RaTE-NER dataset offers a level of granularity not seen in previous datasets, 
      with comprehensive entity annotations within sentences. This enhanced granularity enables to train 
      models for medical entity recognition within our analytical pipeline.
    </p>
    <p>
      <figure>
        <img style="width:600px" src='./resources/data_curation_3.png'>
        <figcaption>Auto-annotation Part (Radiopaedia) in RaTE-NER Dataset.</figcaption>
      </figure>
    </p>
    <br>

    <hr>
    <center> <h2> RaTE-Eval Benchmark </h2> </center>
    <p>
      To effectively evaluate the alignment between automatic evaluation metrics and radiologists' assessments in medical text generation tasks, 
      we have established a comprehensive benchmark, RaTE-Eval, that encompasses three tasks, each with its official test set for fair comparison, 
      as detailed below. The comparison of RaTE-Eval Benchmark and existed radiology report evaluation Benchmark is listed in Table.
    </p>

    <p>
      <figure>
        <img style="width:700px" src='./resources/eval.jpg'>
        <figcaption>RaTE-NER Dataset Statistics: The dataset consists of two data sources: MIMIC-IV and Radiopaedia. 
          # represents specific types of medical entities.  For "Reports" line, the numbers in "()" are number of source reports.   
          For the "Entities" and # lines, the numbers in "()" are counts of non-redundant entities.</figcaption>
      </figure>
    </p>
   
    <p>For more detailed about the subtasks in RaTE-Eval Benchmark, please refer to our  <a href="https://arxiv.org/pdf/2406.16845"> paper</a>.</p>

    <br>
      <hr>
      <center><h2>BibTeX</h2></center>
      <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
	@article{zhao2024ratescore,
	  title={RaTEScore: A Metric for Radiology Report Generation},
	  author={Zhao, Weike and Wu, Chaoyi and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng 
		  and Xie, Weidi},
	  journal={arXiv preprint arXiv:2406.16845},
	  year={2024}
	}
              </code>
          </pre>
        </div>      
      </div>

      
      <br>
<br>
</body>
</html>
